{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84832dd-8165-4c79-aa25-7b437b46ec3d",
   "metadata": {},
   "source": [
    "# Spam detection Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca59833-ccb6-4038-bca3-6d25bc605593",
   "metadata": {},
   "source": [
    "## 1. In order to create a Neural Network for spam detection, one needs to acquire a dataset that highlights both spam and ham messages. Thankfully, the dataset has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cf79f-996a-48e8-b698-174fd09d87aa",
   "metadata": {},
   "source": [
    "## 2. The next step is to prepare the data; this can be done through tensorflow's built in data prcoessing tools, as well as panda and numpy's tools.\n",
    "I imported multiple libraries; pandas for acquiring the dataset, tensorflow for defining the model and its layers, as well as its text processing tools, and numpy and pandas for dataset manipulation. The selected tools will be explained in the following markdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fe3ee40-19cb-4315-beb9-50a7d7a9397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, TextVectorization, GlobalAveragePooling1D, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8dec26-3efd-4a3d-b4fe-a4c59374ccae",
   "metadata": {},
   "source": [
    "Here, I define the location of the dataset, please do not mind the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2f71edb-3d9a-4c69-afee-dac193c87ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Downloads\\\\spam.csv\",encoding='latin1').astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fee61-8f12-4508-a4fc-a244abac4b58",
   "metadata": {},
   "source": [
    "### I then type out \"spam\" and \"spam.describe\" to see what it contains; we then find out it has 5 columns; one for v1, one for v2 and three unnamed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4e8d33f-351a-4c95-a25b-2a6506e215ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        nan   \n",
       "1      ham                      Ok lar... Joking wif u oni...        nan   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        nan   \n",
       "3      ham  U dun say so early hor... U c already then say...        nan   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        nan   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        nan   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        nan   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        nan   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        nan   \n",
       "5571   ham                         Rofl. Its true to its name        nan   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           nan        nan  \n",
       "1           nan        nan  \n",
       "2           nan        nan  \n",
       "3           nan        nan  \n",
       "4           nan        nan  \n",
       "...         ...        ...  \n",
       "5567        nan        nan  \n",
       "5568        nan        nan  \n",
       "5569        nan        nan  \n",
       "5570        nan        nan  \n",
       "5571        nan        nan  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24abb93d-65c7-4130-b661-7b678af50431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        nan   \n",
       "1      ham                      Ok lar... Joking wif u oni...        nan   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        nan   \n",
       "3      ham  U dun say so early hor... U c already then say...        nan   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        nan   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        nan   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        nan   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        nan   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        nan   \n",
       "5571   ham                         Rofl. Its true to its name        nan   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           nan        nan  \n",
       "1           nan        nan  \n",
       "2           nan        nan  \n",
       "3           nan        nan  \n",
       "4           nan        nan  \n",
       "...         ...        ...  \n",
       "5567        nan        nan  \n",
       "5568        nan        nan  \n",
       "5569        nan        nan  \n",
       "5570        nan        nan  \n",
       "5571        nan        nan  \n",
       "\n",
       "[5572 rows x 5 columns]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2299ec-7e70-418a-80cd-5bce471e7ab6",
   "metadata": {},
   "source": [
    "### We know there are excess columns, the responses are in the form of \"spam\" and \"ham\", and that the response and text columns are named v1 and v2 respectively; we would need to \"clean\" the dataset in order for it to be useful.\n",
    "\n",
    "- I start by replacing the **\"spam\"** and **\"ham\"** values with **1** and **0** respectively.\n",
    "- After that, I dropped the **3 unnamed columns**, and renamed the **v1** and **v2** columns with **\"text\"** and **\"response\".**\n",
    "- I then configure the **\"text\"** column to be the **independent variable (x)** and the **\"response\" column to be the dependent variable (y)**\n",
    "- I then split the dataset through sklearn's **train_test_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "813b36fb-81b0-4c41-9e26-8a39ccf27443",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = spam.replace([\"spam\", \"ham\"],[1,0])\n",
    "spam = spam.drop([\"Unnamed: 2\",\"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "spam = spam.rename(columns={\"v1\":'response', \"v2\":'text'})\n",
    "X = spam['text']\n",
    "y = spam['response']\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d18db-46fb-421d-b7f1-61d03237fde7",
   "metadata": {},
   "source": [
    "I then use keras' TextVectorization layer; this essentially translates the words from the spam dataset into integers, from which the model will understand. \n",
    "\n",
    "The values are arbitrary; **vocab_size** just means how many unique words will the vectorization consider. 10,000 felt big enough.\n",
    "**seq_length** is 500 because I want it to handle a maximum of 500 words at once; if my interpretation is correct.\\\n",
    "We also used **'lower_and_strip_punctuation'** to strip the dataset of unnecessary characters, and prevent it from misinterpreting capitalizations \\\n",
    "(i.e. A and a may be the same to us, but the computer might interpret it as two different characters)\n",
    "\n",
    "After defining the layer, we need to give it words that it can transform; we can then use our splitted dataset from earlier; the **train_X.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8175ae3-4ea3-4ce1-97c2-5a064dea6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "seq_length = 500\n",
    "vectorizer_layer = TextVectorization(max_tokens=vocab_size, standardize = 'lower_and_strip_punctuation', output_mode='int', \n",
    " output_sequence_length=seq_length)\n",
    "vectorizer_layer.adapt(train_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430842c3-632d-45bf-830d-ca0a259ddc82",
   "metadata": {},
   "source": [
    "## 3. We can now define the model; as part of the constraints, I will use keras' **Sequential** model.\n",
    "Here, we have 7 layers; \n",
    "1. 1st layer is the input layer; we set it to a shape of 1, because we need it to process every string individually.\n",
    "2. The second layer is the vectorization layer that we made earlier.\n",
    "3. The third layer is the Embedding; this works alongside out vectorizer layer to process our training data.\n",
    "4. Dropout layer is implemented to prevent overfitting; set to 0.2 arbitrarily.\n",
    "5. The GlobalAveragePooling1D processes the output from the Embedding layer.\n",
    "6. the Dense layer is set to 1, as we only need 2 outputs: (0 or 1) I've read somewhere that the best activation for NLP is sigmoid, so I used it here.\n",
    "\n",
    "I then compiled the model using **binary_crossentropy** as loss, since we're only accepting two inputs; and **adam** as my optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ae99bf4-ff76-496b-8f23-4c7795b7f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorizer_layer)\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbba8a4-c3d1-41ba-92e7-6e6712fe2532",
   "metadata": {},
   "source": [
    "## 4. We can now start training the model; \n",
    "I've set the **epochs to 50**, and **verbose to 1** so I can see the cool progress bar.\n",
    "the selected validation_data for this would be the splitted dataset earlier; the **test_X** and its output **test_Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b4dd26d-e20c-40ae-9d59-ad09a55b7b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "446/446 [==============================] - 2s 3ms/step - loss: 0.4492 - accuracy: 0.8658 - val_loss: 0.3879 - val_accuracy: 0.8655\n",
      "Epoch 2/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3899 - accuracy: 0.8661 - val_loss: 0.3842 - val_accuracy: 0.8655\n",
      "Epoch 3/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3838 - accuracy: 0.8661 - val_loss: 0.3804 - val_accuracy: 0.8655\n",
      "Epoch 4/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3778 - accuracy: 0.8661 - val_loss: 0.3761 - val_accuracy: 0.8655\n",
      "Epoch 5/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3748 - accuracy: 0.8661 - val_loss: 0.3712 - val_accuracy: 0.8655\n",
      "Epoch 6/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3690 - accuracy: 0.8661 - val_loss: 0.3654 - val_accuracy: 0.8655\n",
      "Epoch 7/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3614 - accuracy: 0.8661 - val_loss: 0.3586 - val_accuracy: 0.8655\n",
      "Epoch 8/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3541 - accuracy: 0.8661 - val_loss: 0.3505 - val_accuracy: 0.8655\n",
      "Epoch 9/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3454 - accuracy: 0.8661 - val_loss: 0.3408 - val_accuracy: 0.8655\n",
      "Epoch 10/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3324 - accuracy: 0.8661 - val_loss: 0.3292 - val_accuracy: 0.8655\n",
      "Epoch 11/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.3199 - accuracy: 0.8661 - val_loss: 0.3153 - val_accuracy: 0.8655\n",
      "Epoch 12/50\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.3014 - accuracy: 0.8661 - val_loss: 0.2991 - val_accuracy: 0.8655\n",
      "Epoch 13/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.2845 - accuracy: 0.8661 - val_loss: 0.2801 - val_accuracy: 0.8655\n",
      "Epoch 14/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.8678 - val_loss: 0.2617 - val_accuracy: 0.8789\n",
      "Epoch 15/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.2404 - accuracy: 0.8811 - val_loss: 0.2379 - val_accuracy: 0.8816\n",
      "Epoch 16/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9022 - val_loss: 0.2168 - val_accuracy: 0.9139\n",
      "Epoch 17/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1944 - accuracy: 0.9242 - val_loss: 0.1963 - val_accuracy: 0.9247\n",
      "Epoch 18/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1739 - accuracy: 0.9401 - val_loss: 0.1776 - val_accuracy: 0.9336\n",
      "Epoch 19/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1551 - accuracy: 0.9513 - val_loss: 0.1626 - val_accuracy: 0.9489\n",
      "Epoch 20/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1385 - accuracy: 0.9601 - val_loss: 0.1469 - val_accuracy: 0.9489\n",
      "Epoch 21/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1261 - accuracy: 0.9670 - val_loss: 0.1346 - val_accuracy: 0.9543\n",
      "Epoch 22/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9697 - val_loss: 0.1273 - val_accuracy: 0.9507\n",
      "Epoch 23/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.1035 - accuracy: 0.9715 - val_loss: 0.1157 - val_accuracy: 0.9632\n",
      "Epoch 24/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0952 - accuracy: 0.9742 - val_loss: 0.1074 - val_accuracy: 0.9686\n",
      "Epoch 25/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0871 - accuracy: 0.9764 - val_loss: 0.1011 - val_accuracy: 0.9704\n",
      "Epoch 26/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0806 - accuracy: 0.9791 - val_loss: 0.0997 - val_accuracy: 0.9659\n",
      "Epoch 27/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0750 - accuracy: 0.9794 - val_loss: 0.0937 - val_accuracy: 0.9686\n",
      "Epoch 28/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0706 - accuracy: 0.9803 - val_loss: 0.0869 - val_accuracy: 0.9713\n",
      "Epoch 29/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0656 - accuracy: 0.9818 - val_loss: 0.0826 - val_accuracy: 0.9758\n",
      "Epoch 30/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0630 - accuracy: 0.9816 - val_loss: 0.0796 - val_accuracy: 0.9767\n",
      "Epoch 31/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0591 - accuracy: 0.9827 - val_loss: 0.0775 - val_accuracy: 0.9740\n",
      "Epoch 32/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0576 - accuracy: 0.9834 - val_loss: 0.0756 - val_accuracy: 0.9740\n",
      "Epoch 33/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9845 - val_loss: 0.0725 - val_accuracy: 0.9785\n",
      "Epoch 34/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9847 - val_loss: 0.0702 - val_accuracy: 0.9785\n",
      "Epoch 35/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0505 - accuracy: 0.9852 - val_loss: 0.0688 - val_accuracy: 0.9803\n",
      "Epoch 36/50\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0472 - accuracy: 0.9861 - val_loss: 0.0686 - val_accuracy: 0.9785\n",
      "Epoch 37/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 0.0723 - val_accuracy: 0.9749\n",
      "Epoch 38/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0438 - accuracy: 0.9854 - val_loss: 0.0651 - val_accuracy: 0.9830\n",
      "Epoch 39/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0430 - accuracy: 0.9877 - val_loss: 0.0642 - val_accuracy: 0.9794\n",
      "Epoch 40/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0400 - accuracy: 0.9881 - val_loss: 0.0645 - val_accuracy: 0.9794\n",
      "Epoch 41/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9874 - val_loss: 0.0626 - val_accuracy: 0.9821\n",
      "Epoch 42/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0384 - accuracy: 0.9883 - val_loss: 0.0603 - val_accuracy: 0.9839\n",
      "Epoch 43/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0368 - accuracy: 0.9899 - val_loss: 0.0599 - val_accuracy: 0.9830\n",
      "Epoch 44/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9897 - val_loss: 0.0589 - val_accuracy: 0.9839\n",
      "Epoch 45/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0346 - accuracy: 0.9895 - val_loss: 0.0589 - val_accuracy: 0.9821\n",
      "Epoch 46/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 0.0573 - val_accuracy: 0.9848\n",
      "Epoch 47/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0329 - accuracy: 0.9904 - val_loss: 0.0577 - val_accuracy: 0.9839\n",
      "Epoch 48/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0326 - accuracy: 0.9901 - val_loss: 0.0578 - val_accuracy: 0.9857\n",
      "Epoch 49/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9921 - val_loss: 0.0557 - val_accuracy: 0.9848\n",
      "Epoch 50/50\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0295 - accuracy: 0.9921 - val_loss: 0.0558 - val_accuracy: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f91441fd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, epochs=50, verbose=1, validation_data=(test_X, test_y), batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d96d9c-a5cd-4db2-94bd-1fc6b9aac633",
   "metadata": {},
   "source": [
    "## 5. We can evaluate now how the model performed;\n",
    "I wanted to see how much true positives and false negatives the model got versus the false positives and true negatives;\\\n",
    "judging from the results, it looks to be okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1daae6f5-b696-4d68-a360-d9aa4563719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[963,   2],\n",
       "       [ 16, 134]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_test = model.predict(test_X)\n",
    "conf_test = np.around(conf_test)\n",
    "confusion_matrix(test_y,conf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af339c-8185-4e3f-a11c-5faeee24ddda",
   "metadata": {},
   "source": [
    "## 6. The final model here is not representative of the amount of work that needed to be done in order to prepare it; I did go back to modify the model a few times, and this is so far the best I've seen it perform, so I'm keeping it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1925d-33aa-4153-9c63-391c68918792",
   "metadata": {},
   "source": [
    "## 7. After evaluation and tuning. I've given it new data to predict; the output_csv file. \n",
    "1. I used panda's **.read_csv** command; and then asked the model to predict which of the the messages in that new dataset are spam;\n",
    "2. I then rounded off the numbers through numpy's **.around()** command.\n",
    "3. I then created a new array that bases its contents on the previously rounded off predictions: if it is 1, the new array index will contain the word \"spam\", while if it is 0 it contain \"ham\".\n",
    "4. I then write this array to another csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8eb6d78-db43-44cf-bb9d-a0068d1dbd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "spam_out = pd.read_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Downloads\\\\output_spam.csv\",encoding='latin1').astype(\"str\")\n",
    "\n",
    "predictions = model.predict(spam_out['text'])\n",
    "predictions =  np.around(predictions)\n",
    "pred_conv = ['Spam' if i > 0.5 else 'Ham' for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff4c5a84-1b08-47fb-823d-d3ae9da51ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_out = pd.DataFrame(pred_conv, columns=['prediction']).to_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Desktop\\\\billones_spam_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebea381-c4e4-4306-be37-e18c257686c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

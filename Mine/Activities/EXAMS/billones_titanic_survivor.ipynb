{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88f8503-f83c-422e-a68e-f034b2e0883e",
   "metadata": {},
   "source": [
    "# Titanic Survivor Prediction Model\n",
    "#### 1. Data Gathering\n",
    "Before all else, one needs a dataset to process data; since the main question is \"what kind of people were more likely to survive\", I decided to look for passenger data through Google; thankfully, as part of Kaggle's competition, a passenger dataset was provided for those interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc88e64-6803-4ccb-ac6a-4253de47a68c",
   "metadata": {},
   "source": [
    "To begin, i decided to call several libraries such as sklearn's **Naive Bayes**, **linear_model** and **ensemble**.\n",
    "I also called **pandas** and **numpy** for dataset manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1397f275-c727-4877-a38b-57083aa9c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7dad1-2d5a-4e09-b4f7-3209912712e1",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058453f-a0e7-4847-9d8e-e6a43aeeeb2b",
   "metadata": {},
   "source": [
    "In order to preprocess the data, I first need to load it through panda's **read_csv** command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd90c688-7f6b-404e-8b30-a10ba6df4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_survivor = pd.read_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Downloads\\\\train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f9942-5e65-4b16-bfa7-7cb5241ac91b",
   "metadata": {},
   "source": [
    "After accessing, I view its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d02131a-d7a1-4f74-8e4f-035d79d0783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_survivor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e85ff8-de1a-4a5e-a8d4-8ae03e8a51b8",
   "metadata": {},
   "source": [
    "Interpreting the dataset, I've noticed several things for consideration:\n",
    "1. The numeric values are in integer form;\n",
    "2. We have irrelevant (based on my interpretation) data, such as **Ticket**, **PassengerId**, and **Name**\n",
    "3. There are empty cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c99fa-02c2-47d0-81fc-9fae59ab7739",
   "metadata": {},
   "source": [
    "Before I go too deep in maniupulating the preprocessing data, I first need to remove unecessary features; I used the **.drop** command for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4ed112-e345-468a-a3a1-61fe6acfcd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titanic_survivor = titanic_survivor.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cea766-ce11-4175-bc4e-90e1dbf68f09",
   "metadata": {},
   "source": [
    "For this next set of code, I did the following:\n",
    "1. I changed the **male** and **female** with numbers **0** and **1** respectively.\n",
    "2. I filled the empty cells of Embareked with the letter 'S' as it is the most frequent; and replaceD the letters S, C and Q with 3, 1 and 2.\n",
    "3. I then filled the empty \"Age\" cells with the mean age; I also rounded off the values using **.ceil** command, and set it as an integer.\n",
    "4. Finally, I used the same rounding off method from the \"Age\" column TO THE \"Fare\" column, and set its values to be an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e9a7c3-4f71-473a-9772-445477b1cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_survivor = titanic_survivor.replace([\"male\" , \"female\"], [0,1])\n",
    "\n",
    "titanic_survivor[\"Embarked\"] = titanic_survivor[\"Embarked\"].fillna('S')\n",
    "titanic_survivor[\"Embarked\"] = titanic_survivor[\"Embarked\"].replace([\"S\" , \"C\", \"Q\"], [3,1,2])\n",
    "\n",
    "titanic_survivor[\"Age\"] = titanic_survivor[\"Age\"].fillna(titanic_survivor[\"Age\"].mean())\n",
    "titanic_survivor[\"Age\"] = np.ceil(titanic_survivor[\"Age\"])\n",
    "titanic_survivor[\"Age\"] = titanic_survivor[\"Age\"].astype('int')\n",
    "\n",
    "titanic_survivor[\"Fare\"] = np.ceil(titanic_survivor[\"Fare\"])\n",
    "titanic_survivor[\"Fare\"] = titanic_survivor[\"Fare\"].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9555-3c91-48de-bd76-2ab24c79eac4",
   "metadata": {},
   "source": [
    "I then defined which columns are X and which columns are y. Column 0 would be my y as it is teh output, and the reamining columns would be my X.\n",
    "I then split the dataset, with a test size of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d458af-51b3-41ac-ac51-2cb74fce93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_survivor.iloc[:, 1:]\n",
    "y = titanic_survivor.iloc[:, 0]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd73bf-cd3f-497e-987b-ba4b40ac6bd3",
   "metadata": {},
   "source": [
    "#### 3. Choosing a Model\n",
    "As part of the constraints; I selected two models, the Random Forest Classifier and the Gradient Boosting Classifier. I also made a NN through **keras'** Sequential Model.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348b368-e43c-4595-8efe-6978f4131bff",
   "metadata": {},
   "source": [
    "#### 4. Training\n",
    "The split dataset earlier were used to fit to the model; I've divided it into three separate cells for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "505ed4ba-6253-40a6-bc40-2c21407274f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111,   6],\n",
       "       [ 18,  44]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(learning_rate=0.01)\n",
    "model.fit(train_X, train_y)\n",
    "predictions = model.predict(test_X)\n",
    "predictions = np.around(predictions)\n",
    "classification_report(test_y, predictions)\n",
    "confusion_matrix(test_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74661a14-4d40-4a0f-8ad5-ee1b1f62c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.59217877094973%\n"
     ]
    }
   ],
   "source": [
    "print(str(accuracy_score(test_y,predictions)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5865cd4-bba0-453d-8dc9-aec881b7022b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110,   7],\n",
       "       [ 18,  44]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=4, random_state=5)\n",
    "model.fit(train_X, train_y)\n",
    "predictions = model.predict(test_X)\n",
    "predictions = np.around(predictions)\n",
    "classification_report(test_y, predictions)\n",
    "confusion_matrix(test_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e580995a-177f-4274-bac9-971b7c07a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.59217877094973%\n"
     ]
    }
   ],
   "source": [
    "print(str(accuracy_score(test_y,predictions)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb3b07f0-562b-4804-b5b8-4aac2c8f0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 1s 3ms/step - loss: 0.8107 - accuracy: 0.6222 - val_loss: 0.5821 - val_accuracy: 0.7374\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6418 - accuracy: 0.6657 - val_loss: 0.5283 - val_accuracy: 0.7654\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6307 - accuracy: 0.6713 - val_loss: 0.5190 - val_accuracy: 0.7877\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.6952 - val_loss: 0.5272 - val_accuracy: 0.7542\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5906 - accuracy: 0.6938 - val_loss: 0.5139 - val_accuracy: 0.7709\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.6938 - val_loss: 0.5328 - val_accuracy: 0.7374\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7037 - val_loss: 0.5007 - val_accuracy: 0.7765\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.7205 - val_loss: 0.4641 - val_accuracy: 0.7877\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5296 - accuracy: 0.7584 - val_loss: 0.4516 - val_accuracy: 0.8212\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.7500 - val_loss: 0.5261 - val_accuracy: 0.7654\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.7584 - val_loss: 0.4484 - val_accuracy: 0.7877\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7781 - val_loss: 0.5174 - val_accuracy: 0.7542\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7851 - val_loss: 0.4810 - val_accuracy: 0.7877\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.7851 - val_loss: 0.4375 - val_accuracy: 0.8045\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.7767 - val_loss: 0.4322 - val_accuracy: 0.8101\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7949 - val_loss: 0.4122 - val_accuracy: 0.8268\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.8020 - val_loss: 0.4576 - val_accuracy: 0.7989\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.8062 - val_loss: 0.4046 - val_accuracy: 0.8101\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7893 - val_loss: 0.4845 - val_accuracy: 0.7765\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7978 - val_loss: 0.4301 - val_accuracy: 0.8101\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.8020 - val_loss: 0.4277 - val_accuracy: 0.8156\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7893 - val_loss: 0.4044 - val_accuracy: 0.8156\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.7963 - val_loss: 0.4246 - val_accuracy: 0.8212\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7963 - val_loss: 0.3828 - val_accuracy: 0.8212\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4507 - accuracy: 0.7949 - val_loss: 0.4331 - val_accuracy: 0.8212\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.8090 - val_loss: 0.4035 - val_accuracy: 0.8045\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.8062 - val_loss: 0.4206 - val_accuracy: 0.7989\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.4267 - val_accuracy: 0.8380\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7935 - val_loss: 0.3876 - val_accuracy: 0.8324\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4392 - accuracy: 0.8076 - val_loss: 0.4126 - val_accuracy: 0.8268\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8062 - val_loss: 0.4075 - val_accuracy: 0.8212\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.7935 - val_loss: 0.4002 - val_accuracy: 0.8156\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8146 - val_loss: 0.4192 - val_accuracy: 0.8268\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8104 - val_loss: 0.3972 - val_accuracy: 0.8380\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.8202 - val_loss: 0.4277 - val_accuracy: 0.8045\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8062 - val_loss: 0.3989 - val_accuracy: 0.8101\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4545 - accuracy: 0.7865 - val_loss: 0.3959 - val_accuracy: 0.8380\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.8174 - val_loss: 0.4993 - val_accuracy: 0.7933\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8118 - val_loss: 0.4016 - val_accuracy: 0.8156\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8104 - val_loss: 0.4050 - val_accuracy: 0.8045\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8132 - val_loss: 0.4092 - val_accuracy: 0.8324\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.8090 - val_loss: 0.4048 - val_accuracy: 0.8436\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.8062 - val_loss: 0.4406 - val_accuracy: 0.8101\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8020 - val_loss: 0.4125 - val_accuracy: 0.8436\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8146 - val_loss: 0.4300 - val_accuracy: 0.8156\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.8188 - val_loss: 0.3989 - val_accuracy: 0.8324\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8174 - val_loss: 0.4127 - val_accuracy: 0.8380\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8146 - val_loss: 0.3957 - val_accuracy: 0.8324\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8034 - val_loss: 0.4031 - val_accuracy: 0.8436\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.4336 - val_accuracy: 0.8268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c93c20b760>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=50, verbose=1, validation_data=(test_X, test_y), batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f96f8-eaf5-4c1c-ab25-2a5fe146ed64",
   "metadata": {},
   "source": [
    "#### 5. Evaluation\n",
    "To evaluate my model, I used **sklearn's** **confusion_matrix** command; I then asked it to compare its prediction (of test_X acquired earlier) to the ACTUAL test_y results.  I also printed out its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e821837a-ca61-47ac-be5c-2e388db3a157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[106,  11],\n",
       "       [ 19,  43]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_X)\n",
    "predictions = np.around(predictions)\n",
    "classification_report(test_y, predictions)\n",
    "confusion_matrix(test_y,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6df1ffb-b98c-40a5-86d2-0e6d5e402bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.24022346368714%\n"
     ]
    }
   ],
   "source": [
    "print(str(accuracy_score(test_y,predictions)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b19581-284a-4809-a919-20aeca9d0568",
   "metadata": {},
   "source": [
    "#### 6. Hyperparameter Tuning\n",
    "For the Gradient Boosting Classifier, I modified its learning rate, and set it to 0.01; while the Random Forest Classifier had modified **max_depth** and **random_state**. the values are acquired through trial and error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c665ba0-8c50-4f17-b51c-b98d22c60970",
   "metadata": {},
   "source": [
    "#### 7. Prediction\n",
    "Alongside Kaggle's training dataset they graced us with a test dataset, which I prepped for prediction below; following the same format of dropping the unnecessary columns and changing the floating point values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6c2f8c86-14bd-4e30-a488-ee0e0ce13a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 846us/step\n"
     ]
    }
   ],
   "source": [
    "titanic_s = pd.read_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Downloads\\\\test (1).csv\",encoding='latin1')\n",
    "\n",
    "titanic_s = titanic_s.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "titanic_s = titanic_s.replace([\"male\" , \"female\"], [0,1])\n",
    "\n",
    "titanic_s[\"Embarked\"] = titanic_s[\"Embarked\"].fillna('S')\n",
    "titanic_s[\"Embarked\"] = titanic_s[\"Embarked\"].replace([\"S\" , \"C\", \"Q\"], [3,1,2])\n",
    "\n",
    "titanic_s[\"Age\"] = titanic_s[\"Age\"].fillna(titanic_s[\"Age\"].mean())\n",
    "titanic_s[\"Age\"] = np.ceil(titanic_s[\"Age\"])\n",
    "titanic_s[\"Age\"] = titanic_s[\"Age\"].astype('int')\n",
    "\n",
    "titanic_s[\"Fare\"] = np.ceil(titanic_s[\"Fare\"])\n",
    "titanic_s[\"Fare\"] = titanic_s[\"Fare\"].fillna(titanic_s[\"Fare\"].mean())\n",
    "titanic_s[\"Fare\"] = titanic_s[\"Fare\"].astype('int')\n",
    "\n",
    "predictions = model.predict(titanic_s)\n",
    "predictions =  np.around(predictions)\n",
    "pred_conv = ['Survived' if i > 0.5 else 'Dead' for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8aadee67-9770-4010-86f4-bcf3cc9b0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_out = pd.DataFrame(pred_conv, columns=['prediction']).to_csv(\"C:\\\\Users\\\\Dingus-Elite\\\\Desktop\\\\billones_titanic_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af64fb1c-bd54-4bd9-9846-83727e1f8dcc",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "Across all the three models, the best performing are the Random Forest Classifier and the Gradient Boosting CLassifier, at 86%; I had several attempts at making sure my NN acquires the same results but it seems to cap at 83%. I suspect that with time and experience, I might be able to design a NN that somewhat comes close to the two models described above.\n",
    "\n",
    "The dataset was really messy; with floating points on age, ridiculous amount of NaN cells that needed to be filled; there were severaal suggestions as to how they may be handled, one is to fill with with 0 (I can't imagine someone would be 0 years old and board a ship on their own) and that you can fill them with averages. I decided to do the latter. I also took notice of two things; People who embarked from South Hampton are more likely to have records, not necessarily survived. Same goes with the Pclass, while there are more 3rd class passenger on the ship, it does not necessarily dictate their survival.\n",
    "\n",
    "Curiously, as I looked further into the dataset, I noticed that there are some information that MIGHT be useful for training. Cabin Numbers dictate which part of the titanic are they on, and the passenger names contain the marital status which is useful; you can be a Mr. or Mrs. but not have a spouse, or not have a child, or both. \n",
    "\n",
    "This might be one of the more hands-on activities in machine learning that I've done so far; it is interesting to see how different manipulations in the dataset affect the accuracy of the model. This is why I believe the 2nd Step is so important; no dataset is ever whole, or clean, and so its up to the programmers to figure out how to scrub it out and figure out how to best approach the dataset. With that in mind; Machine Learning is not just about feeding the dataset to an algorithm; it takes careful consideration of every parameter you can modify; and that parameter in turn, defines how the model will perform.\n",
    "\n",
    "While I did some dataset cleaning here, to an experienced data scientist it might just be like brushing one's teeth, rather than scrubbing the toilet.\\\n",
    "It is clear that I still have much to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ccca7-6f34-4ba0-a158-c9e7125458d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
